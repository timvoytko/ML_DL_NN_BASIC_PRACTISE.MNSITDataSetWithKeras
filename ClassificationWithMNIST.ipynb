{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\conda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "E:\\conda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\conda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\conda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\conda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\conda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\conda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data\n",
    "df_test = pd.read_csv('fashion-mnist_test.csv')\n",
    "df_train = pd.read_csv('fashion-mnist_train.csv')\n",
    "y_train = df_train.label\n",
    "x_train = df_train.drop(['label'], axis=1)/255\n",
    "y_test = df_test.label\n",
    "x_test = (df_test.drop(['label'], axis=1))/255\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\conda\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "# build a nn for solving next task:statistical classification (solving method: logistic regression)\n",
    "# build a logistic regression like a NN without any hide layers\n",
    "# optimizer = stochastic gradient descent, metric - accuracy\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu', input_shape=(784,)))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 14s 238us/sample - loss: 0.3104 - acc: 0.8910 - val_loss: 0.3400 - val_acc: 0.8823\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 14s 236us/sample - loss: 0.3060 - acc: 0.8919 - val_loss: 0.3347 - val_acc: 0.8803\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 14s 236us/sample - loss: 0.3023 - acc: 0.8932 - val_loss: 0.3307 - val_acc: 0.8814\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 14s 235us/sample - loss: 0.2996 - acc: 0.8948 - val_loss: 0.3552 - val_acc: 0.8706\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 14s 237us/sample - loss: 0.2957 - acc: 0.8961 - val_loss: 0.3253 - val_acc: 0.8842\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 14s 239us/sample - loss: 0.2920 - acc: 0.8971 - val_loss: 0.3242 - val_acc: 0.8864\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 14s 238us/sample - loss: 0.2892 - acc: 0.8982 - val_loss: 0.3257 - val_acc: 0.8835\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 14s 237us/sample - loss: 0.2848 - acc: 0.8994 - val_loss: 0.3262 - val_acc: 0.8833\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 14s 238us/sample - loss: 0.2824 - acc: 0.9002 - val_loss: 0.3275 - val_acc: 0.8839\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 14s 237us/sample - loss: 0.2788 - acc: 0.9016 - val_loss: 0.3153 - val_acc: 0.8858\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.3153 - acc: 0.8858\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 5544."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Tensorboard as a Keras callback\n",
    "# fit model with tensorboard on 10 epochs\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), callbacks=[tensorboard_callback])\n",
    "loss, accur = model.evaluate(x_test, y_test)\n",
    "%load_ext tensorboard\n",
    "logs_base_dir = \"./logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "%tensorboard --logdir {logs_base_dir}  --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3616), started 0:08:51 ago. (Use '!kill 3616' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x14b1535e8c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if you got next error:\n",
    "# ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid (pid)\n",
    "# uncomment next stroke:\n",
    "# %tensorboard --logdir {logs_base_dir}  --host localhost\n",
    "# or you can find screenshot of this board in my folder with name TensorBoardScreen.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build full connnected NN with same optimizer and 2 hiden layers\n",
    "model1 = tf.keras.models.Sequential()\n",
    "model1.add(tf.keras.layers.Dense(256, activation='relu', input_shape=(784,)))\n",
    "model1.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model1.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.7284 - acc: 0.7562\n",
      "10000/10000 [==============================] - 1s 50us/sample - loss: 0.5150 - acc: 0.8194\n",
      "[0.5150360456466675, 0.8194]\n"
     ]
    }
   ],
   "source": [
    "model1.fit(x_train, y_train)\n",
    "print(model1.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, in this model I have accuracy 0.82(on test), so it is less than accuracy from previous model(0.89), but also good\n",
    "# I think , that's all because: new model is 'more' retrained, because we have 2 hiden layers with big number of neurons\n",
    "# and nothing more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data for CNN\n",
    "xarr_train = ((df_train.drop(['label'], axis=1)/255).to_numpy()).reshape(xarr_train.shape[0],28,28,1)\n",
    "xarr_test = (df_test.drop(['label'], axis=1)/255).to_numpy().reshape(xarr_test.shape[0],28,28,1)\n",
    "yarr_train = df_train.label.to_numpy()\n",
    "yarr_test = df_test.label.to_numpy()\n",
    "yarr_train = tf.keras.utils.to_categorical(yarr_train)\n",
    "yarr_test = tf.keras.utils.to_categorical(yarr_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple CNN modele \n",
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Convolution2D(32,(3,3),input_shape=(28,28,1), activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 29s 480us/sample - loss: 0.7030 - acc: 0.7534 - loss:\n",
      "10000/10000 [==============================] - 1s 147us/sample - loss: 0.4897 - acc: 0.8261\n",
      "[0.4897075669288635, 0.8261]\n"
     ]
    }
   ],
   "source": [
    "cnn.fit(xarr_train, yarr_train)\n",
    "print(cnn.evaluate(xarr_test, yarr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now, accuracy(on test) is bigger than previous one, I think CNN methods are more suitable for solving this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some layers for this modele (avoiding problemes, I prepared rebuild model)\n",
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Convolution2D(32,(3,3),input_shape=(28,28,1), activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(tf.keras.layers.Convolution2D(64,(3,3), activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 44s 728us/sample - loss: 0.8819 - acc: 0.6844\n",
      "10000/10000 [==============================] - 2s 205us/sample - loss: 0.5932 - acc: 0.7812\n",
      "[0.5932328998565674, 0.7812]\n"
     ]
    }
   ],
   "source": [
    "cnn.fit(xarr_train, yarr_train)\n",
    "print(cnn.evaluate(xarr_test, yarr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that accuracy(on test) is less than acc from previous version of CNN, so thats also because we have more layers\n",
    "# and there was a chance that this model will be overfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some Batch Normalization layers in first and second versions of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 106s 2ms/sample - loss: 0.4048 - acc: 0.8541\n",
      "10000/10000 [==============================] - 2s 241us/sample - loss: 0.3222 - acc: 0.8885\n",
      "[0.32218770798444746, 0.8885]\n"
     ]
    }
   ],
   "source": [
    "#1 \n",
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Convolution2D(32,(3,3),input_shape=(28,28,1), activation='relu'))\n",
    "cnn.add(tf.keras.layers.BatchNormalization())\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(), metrics=['accuracy'])\n",
    "cnn.fit(xarr_train, yarr_train)\n",
    "print(cnn.evaluate(xarr_test, yarr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 150s 2ms/sample - loss: 0.4341 - acc: 0.8429\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 0.3961 - acc: 0.8522\n",
      "[0.39611381821632385, 0.8522]\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.layers.Convolution2D(32,(3,3),input_shape=(28,28,1), activation='relu'))\n",
    "cnn.add(tf.keras.layers.BatchNormalization())\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(tf.keras.layers.Convolution2D(64,(3,3), activation='relu'))\n",
    "cnn.add(tf.keras.layers.BatchNormalization())\n",
    "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(), metrics=['accuracy'])\n",
    "cnn.fit(xarr_train, yarr_train)\n",
    "print(cnn.evaluate(xarr_test, yarr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we see that Batch Normalization layers are really helpful with this data on this model\n",
    "# accuracy in first CNN with batch is the biggest one from this work\n",
    "# and also we see that accuracy increased in second cnn\n",
    "# and now we have really good values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
